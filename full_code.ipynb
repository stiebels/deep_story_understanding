  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The dataset consists of 45502 instances, each consisting of 5 sentences. The system aims to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:\n",
    "\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp.\n",
    "    Jan decided to get a new lamp.\n",
    "    Jan's lamp broke.\n",
    "\n",
    "your system needs to provide an answer in the following form:\n",
    "\n",
    "    2\t3\t4\t1\t0\n",
    "\n",
    "where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So \"`2`\" for \"`He went to the store.`\" means that this sentence should come 3rd in the correctly ordered target story. In This particular example, this order of indices corresponds to the following target story:\n",
    "\n",
    "    Jan's lamp broke.\n",
    "    Jan decided to get a new lamp.\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change, move or copy it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:56.249298",
     "start_time": "2016-12-20T12:04:54.376398"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.110195",
     "start_time": "2016-12-20T12:04:56.251082"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.134033",
     "start_time": "2016-12-20T12:04:57.115270"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order': [3, 2, 1, 0, 4],\n",
       " 'story': ['His parents understood and decided to make a change.',\n",
       "  'The doctors told his parents it was unhealthy.',\n",
       "  'Dan was overweight as well.',\n",
       "  \"Dan's parents were overweight.\",\n",
       "  'They got themselves and Dan on a diet.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train + data_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Code for Porter Stemmer from Vivake Gupta, 2008: https://tartarus.org/martin/PorterStemmer/index.html | https://tartarus.org/martin/PorterStemmer/python.txt\n",
    "\n",
    "class PorterStemmer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.b = \"\"\n",
    "        self.k = 0\n",
    "        self.k0 = 0\n",
    "        self.j = 0\n",
    "\n",
    "    def cons(self, i):\n",
    "        if self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or self.b[i] == 'o' or self.b[i] == 'u':\n",
    "            return 0\n",
    "        if self.b[i] == 'y':\n",
    "            if i == self.k0:\n",
    "                return 1\n",
    "            else:\n",
    "                return (not self.cons(i - 1))\n",
    "        return 1\n",
    "\n",
    "    def m(self):\n",
    "        n = 0\n",
    "        i = self.k0\n",
    "        while 1:\n",
    "            if i > self.j:\n",
    "                return n\n",
    "            if not self.cons(i):\n",
    "                break\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "        while 1:\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "            n = n + 1\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if not self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "\n",
    "    def vowelinstem(self):\n",
    "        for i in range(self.k0, self.j + 1):\n",
    "            if not self.cons(i):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def doublec(self, j):\n",
    "        if j < (self.k0 + 1):\n",
    "            return 0\n",
    "        if (self.b[j] != self.b[j-1]):\n",
    "            return 0\n",
    "        return self.cons(j)\n",
    "\n",
    "    def cvc(self, i):\n",
    "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
    "            return 0\n",
    "        ch = self.b[i]\n",
    "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def ends(self, s):\n",
    "        length = len(s)\n",
    "        if s[length - 1] != self.b[self.k]:\n",
    "            return 0\n",
    "        if length > (self.k - self.k0 + 1):\n",
    "            return 0\n",
    "        if self.b[self.k-length+1:self.k+1] != s:\n",
    "            return 0\n",
    "        self.j = self.k - length\n",
    "        return 1\n",
    "\n",
    "    def setto(self, s):\n",
    "        length = len(s)\n",
    "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
    "        self.k = self.j + length\n",
    "\n",
    "    def r(self, s):\n",
    "        if self.m() > 0:\n",
    "            self.setto(s)\n",
    "\n",
    "    def step1ab(self):\n",
    "        if self.b[self.k] == 's':\n",
    "            if self.ends(\"sses\"):\n",
    "                self.k = self.k - 2\n",
    "            elif self.ends(\"ies\"):\n",
    "                self.setto(\"i\")\n",
    "            elif self.b[self.k - 1] != 's':\n",
    "                self.k = self.k - 1\n",
    "        if self.ends(\"eed\"):\n",
    "            if self.m() > 0:\n",
    "                self.k = self.k - 1\n",
    "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
    "            self.k = self.j\n",
    "            if self.ends(\"at\"):   self.setto(\"ate\")\n",
    "            elif self.ends(\"bl\"): self.setto(\"ble\")\n",
    "            elif self.ends(\"iz\"): self.setto(\"ize\")\n",
    "            elif self.doublec(self.k):\n",
    "                self.k = self.k - 1\n",
    "                ch = self.b[self.k]\n",
    "                if ch == 'l' or ch == 's' or ch == 'z':\n",
    "                    self.k = self.k + 1\n",
    "            elif (self.m() == 1 and self.cvc(self.k)):\n",
    "                self.setto(\"e\")\n",
    "\n",
    "    def step1c(self):\n",
    "        if (self.ends(\"y\") and self.vowelinstem()):\n",
    "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
    "\n",
    "    def step2(self):\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
    "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"bli\"):       self.r(\"ble\")\n",
    "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
    "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
    "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
    "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
    "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"alism\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
    "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
    "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
    "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
    "        elif self.b[self.k - 1] == 'g':\n",
    "            if self.ends(\"logi\"):      self.r(\"log\")\n",
    "\n",
    "    def step3(self):\n",
    "        if self.b[self.k] == 'e':\n",
    "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
    "            elif self.ends(\"ative\"):   self.r(\"\")\n",
    "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
    "        elif self.b[self.k] == 'i':\n",
    "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
    "        elif self.b[self.k] == 'l':\n",
    "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
    "            elif self.ends(\"ful\"):     self.r(\"\")\n",
    "        elif self.b[self.k] == 's':\n",
    "            if self.ends(\"ness\"):      self.r(\"\")\n",
    "\n",
    "    def step4(self):\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"al\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"ance\"): pass\n",
    "            elif self.ends(\"ence\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"er\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'i':\n",
    "            if self.ends(\"ic\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"able\"): pass\n",
    "            elif self.ends(\"ible\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'n':\n",
    "            if self.ends(\"ant\"): pass\n",
    "            elif self.ends(\"ement\"): pass\n",
    "            elif self.ends(\"ment\"): pass\n",
    "            elif self.ends(\"ent\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
    "            elif self.ends(\"ou\"): pass\n",
    "            # takes care of -ous\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"ism\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"ate\"): pass\n",
    "            elif self.ends(\"iti\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'u':\n",
    "            if self.ends(\"ous\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'v':\n",
    "            if self.ends(\"ive\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'z':\n",
    "            if self.ends(\"ize\"): pass\n",
    "            else: return\n",
    "        else:\n",
    "            return\n",
    "        if self.m() > 1:\n",
    "            self.k = self.j\n",
    "\n",
    "    def step5(self):\n",
    "        self.j = self.k\n",
    "        if self.b[self.k] == 'e':\n",
    "            a = self.m()\n",
    "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
    "                self.k = self.k - 1\n",
    "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
    "            self.k = self.k -1\n",
    "\n",
    "    def stem(self, p, i, j):\n",
    "        self.b = p\n",
    "        self.k = j\n",
    "        self.k0 = i\n",
    "        if self.k <= self.k0 + 1:\n",
    "            return self.b\n",
    "        self.step1ab()\n",
    "        self.step1c()\n",
    "        self.step2()\n",
    "        self.step3()\n",
    "        self.step4()\n",
    "        self.step5()\n",
    "        return self.b[self.k0:self.k+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "use_stemmer = True\n",
    "\n",
    "if use_stemmer == True:\n",
    "    pstem = PorterStemmer()\n",
    "\n",
    "def tokenize(input): # [SS]\n",
    "    b = split(input)\n",
    "    c = replace(b)\n",
    "    d = strip_numbers(c)\n",
    "    e = remove_short(d)\n",
    "    #f = remove_stop(e)\n",
    "    g = tolower(e)\n",
    "    if use_stemmer == True:\n",
    "        h = stemit(g, pstem)\n",
    "        return h\n",
    "    else:\n",
    "        return e\n",
    "\n",
    "\n",
    "def split(input, excl = False):\n",
    "    a = input.split(' ')\n",
    "    \n",
    "    a_new = []\n",
    "    for word in a:\n",
    "        \n",
    "        if ('n\\'t') in word:\n",
    "            if ('won\\'t') in word:\n",
    "                b = 'will'\n",
    "                a_new.append(b)\n",
    "                a_new.append('not')\n",
    "            else:        \n",
    "                b = word[:-3]\n",
    "                a_new.append(b)\n",
    "                a_new.append('not')\n",
    "        elif ('\\'ll') in word:\n",
    "            b = word[:-3]\n",
    "            a_new.append(b)\n",
    "            a_new.append('will')\n",
    "        elif ('\\'m') in word:\n",
    "            b = word[:-2]\n",
    "            a_new.append(b)\n",
    "            a_new.append('am')\n",
    "        elif ('\\'d') in word:\n",
    "            b = word[:-2]\n",
    "            a_new.append(b)\n",
    "            a_new.append('would')\n",
    "        elif ('\\'s') in word:\n",
    "            b = word[:-2]\n",
    "            a_new.append(b)\n",
    "            a_new.append('\\'s')\n",
    "        elif ('\\'re') in word:\n",
    "            b = word[:-3]\n",
    "            a_new.append(b)\n",
    "            a_new.append('are')\n",
    "        else:\n",
    "            a_new.append(word)\n",
    "\n",
    "    sent_string = \" \".join(str(x) for x in a_new)\n",
    "    #print(a_new)\n",
    "    b = re.split(r'[`\\-=~@#$%^&*()_+\\[\\]{};\\\\:\"|<,/<>\\s]', sent_string)\n",
    "    #print(b)\n",
    "\n",
    "    sent = []\n",
    "    for word in b:\n",
    "        if (word != ' ') and (word != ''):\n",
    "            if word.endswith('?'):\n",
    "                b = word[:-1]\n",
    "                sent.append(b)\n",
    "                #sent.append('?')\n",
    "            elif word.endswith('!'):\n",
    "                b = word[:-1]\n",
    "                sent.append(b)\n",
    "                #sent.append('!')\n",
    "            elif word.endswith('.'):\n",
    "                b = word[:-1]\n",
    "                sent.append(b)\n",
    "                #sent.append('.')\n",
    "            else:\n",
    "                sent.append(word)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def tolower(input):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        sent.append(word.lower())\n",
    "    return sent\n",
    "\n",
    "    \n",
    "def remove_stop(input):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        if word not in stop_words.ENGLISH_STOP_WORDS:\n",
    "            sent.append(word)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def replace(input):\n",
    "    sent = []\n",
    "                \n",
    "    for word in input:\n",
    "        a = ''.join(e for e in word if e.isalnum())\n",
    "        sent.append(a)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def strip_numbers(input):\n",
    "    sent = []\n",
    "                \n",
    "    for word in input:\n",
    "        if bool(re.search(r'\\d', word)) == False:\n",
    "            sent.append(word)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def remove_short(input):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        if len(word) > 1:\n",
    "            sent.append(word)\n",
    "    return sent\n",
    "\n",
    "def stemit(input, stemmer):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        sent.append(pstem.stem(word, 0, len(word)-1))\n",
    "    return sent\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline(data, vocab=None, max_sent_len_=None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "\n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_orders = []\n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        for sentence in instance['story']:\n",
    "            sent = []\n",
    "            tokenized = tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                if not is_ext_vocab and token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                if token not in vocab:\n",
    "                    token_id = vocab['<OOV>']\n",
    "                else:\n",
    "                    token_id = vocab[token]\n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "        data_sentences.append(sents)\n",
    "        data_orders.append(instance['order'])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "\n",
    "    return out_sentences, out_orders, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.842961",
     "start_time": "2016-12-20T12:04:57.136946"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# convert train set to integer IDs\n",
    "train_stories, train_orders, vocab = pipeline(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.925263",
     "start_time": "2016-12-20T12:04:59.844598"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# get the length of the longest sentence\n",
    "max_sent_len = train_stories.shape[2]\n",
    "\n",
    "# convert dev set to integer IDs, based on the train vocabulary and max_sent_len\n",
    "dev_stories, dev_orders, _ = pipeline(data_dev, vocab=vocab, max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.966529",
     "start_time": "2016-12-20T12:04:59.956638"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "input_size = 27 # dimension of word embeddings\n",
    "hidden_size = 95 # \n",
    "num_stacked_rnns = 4 # [SS] - Specify number of stacked RNNs\n",
    "\n",
    "BATCH_SIZE = 130\n",
    "global_dropout = 1.0 # [SS] - specify dropout KEEP probability\n",
    "reg_lmbda = 0.008171001 # [SS] - l2 regularization lambda\n",
    "activation_ff = tf.nn.relu\n",
    "use_pre_trained_embeddings = False\n",
    "\n",
    "checkpoints = [int(n/30*(len(data_train)/BATCH_SIZE)) for n in range(1,30)]\n",
    "dev_accuracies = []\n",
    "\n",
    "# Optimizer\n",
    "optimizer = 'adam'\n",
    "learn_rate_base = 0.005\n",
    "learn_rate_discount = .96756051\n",
    "\n",
    "# Gradient Clipping\n",
    "grad_clipping = False\n",
    "clip_high = 1\n",
    "clip_low = -1\n",
    "\n",
    "early_stopping = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SS - Added whole cell for implementing different optimizers and gradient clipping\n",
    "\n",
    "def optimize(loss, clip_low, clip_high, learn_rate, optimizer, grad_clipping=False, momentum=1):\n",
    "    \n",
    "    # Optimizer selection\n",
    "    if optimizer=='adam':\n",
    "        opt_op = tf.train.AdamOptimizer(learn_rate)\n",
    "    elif optimizer=='adagrad':\n",
    "        opt_op = tf.train.AdagradOptimizer(learn_rate)\n",
    "    elif optimizer=='adadelta':\n",
    "        opt_op = tf.train.AdadeltaOptimizer(learn_rate)\n",
    "    elif optimizer=='momentum':\n",
    "        print('Momentum currently set to '+str(momentum))\n",
    "        opt_op = tf.train.MomentumOptimizer(learning_rate=learn_rate, momentum=momentum)\n",
    "    \n",
    "    if grad_clipping == True:\n",
    "        # Gradient Clipping [SS]\n",
    "        gvs = opt_op.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, clip_low, clip_high), var) for grad, var in gvs]\n",
    "        return opt_op.apply_gradients(capped_gvs)\n",
    "    else:\n",
    "        return opt_op.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:00.995336",
     "start_time": "2016-12-20T12:04:59.968153"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf.set_random_seed(1) # SEED\n",
    "np.random.seed(1) # SEED\n",
    "\n",
    "# Bidirectional RNN sequence lengths\n",
    "seq_len_batch = np.full_like(np.arange(BATCH_SIZE), max_sent_len)\n",
    "seq_len_train = np.full_like(np.arange(len(data_train)), max_sent_len)\n",
    "seq_len_dev = np.full_like(np.arange(len(data_dev)), max_sent_len)\n",
    "\n",
    "target_size = 5 # num classes\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "train_size = len(data_train) #JC\n",
    "training_indices = list(range(train_size)) #JC\n",
    "\n",
    "## Placeholders\n",
    "dropout_pl = tf.placeholder(tf.float32, shape=(None))\n",
    "story_pl = tf.placeholder(tf.int64, [None, None, None], \"story\")\n",
    "labels_pl = tf.placeholder(tf.int64, [None, None], \"labels\")\n",
    "seq_len = tf.placeholder(tf.int64, [None])\n",
    "batch_size = tf.shape(story_pl)[0]\n",
    "learn_rate = tf.placeholder(tf.float32, None)\n",
    "\n",
    "# Word Embeddings\n",
    "# if use_pre_trained_embeddings:\n",
    "#     W = tf.get_variable(name=\"W\", shape=embeddings_list.shape, initializer=tf.constant_initializer(embeddings_list), trainable=False)\n",
    "# else:\n",
    "#     W = tf.get_variable(\"W\", [vocab_size, input_size], initializer=tf.random_uniform_initializer(-1, 1))\n",
    "\n",
    "W = tf.get_variable(\"W\", [vocab_size, input_size], initializer=tf.random_uniform_initializer(-1, 1))\n",
    "    \n",
    "sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story_pl)]\n",
    "story_embedded = [tf.nn.embedding_lookup(W, sent) for sent in sentences]\n",
    "\n",
    "with tf.variable_scope(\"encoder\") as varscope:\n",
    "    tf.set_random_seed(1) # SEED\n",
    "    np.random.seed(1) # SEED\n",
    "    \n",
    "    # Bidirectional Multi-Layer RNN with dropout\n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "    cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, input_keep_prob=global_dropout, output_keep_prob=global_dropout) # [SS]\n",
    "    cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, input_keep_prob=global_dropout, output_keep_prob=global_dropout) # [SS]\n",
    "    # SS - introduced stacked RNN\n",
    "    cell_fw = tf.nn.rnn_cell.MultiRNNCell([cell_fw] * num_stacked_rnns, state_is_tuple=True)\n",
    "    cell_bw = tf.nn.rnn_cell.MultiRNNCell([cell_bw] * num_stacked_rnns, state_is_tuple=True)\n",
    "    cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, output_keep_prob=global_dropout)\n",
    "    cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, output_keep_prob=global_dropout)\n",
    "        \n",
    "    sen_final_state = [None] * 5\n",
    "    sen_h = [None] * 5\n",
    "    outputs = [None] * 5\n",
    "    sen_h_fw = [None] * 5\n",
    "    sen_h_bw = [None] * 5\n",
    "    \n",
    "    for i in range(target_size):\n",
    "        _,sen_final_state[i] =\\\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=story_embedded[i],\n",
    "                                        dtype=tf.float32, sequence_length=seq_len) \n",
    "\n",
    "        sen_h_fw[i], sen_h_bw[i] = sen_final_state[i]\n",
    "        sen_h_fw[i] = sen_h_fw[i][-1].h\n",
    "\n",
    "        varscope.reuse_variables()\n",
    "\n",
    "\n",
    "h_fw = tf.concat(1, sen_h_fw)\n",
    "\n",
    "# [batch_size x target_size]\n",
    "relu_layer = tf.contrib.layers.fully_connected(h_fw, 5 * input_size, activation_fn=activation_ff)\n",
    "logits_flat_fw = tf.contrib.layers.fully_connected(relu_layer, 5 * target_size, activation_fn=None)    # [batch_size x 5*target_size]\n",
    "\n",
    "logits = tf.reshape((logits_flat_fw), [-1, 5, target_size])   \n",
    "\n",
    "\n",
    "tf_vars = tf.trainable_variables() \n",
    "l2_loss = tf.mul(tf.add_n([tf.nn.l2_loss(v) for v in tf_vars if 'bias' not in v.name ]), reg_lmbda)\n",
    "unreg_loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_pl))\n",
    "loss = tf.add(unreg_loss, l2_loss, name='loss')\n",
    "\n",
    "run_optimizer = optimize(loss, clip_low, clip_high, learn_rate, optimizer, grad_clipping) # [SS]\n",
    "\n",
    "unpacked_logits = [tensor for tensor in tf.unpack(logits, axis=1)]\n",
    "softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "softmaxed_logits = tf.pack(softmaxes, axis=1)\n",
    "predict = tf.arg_max(softmaxed_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.615600",
     "start_time": "2016-12-20T12:05:01.186008"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "```python\n",
    "with tf.Session() as sess:\n",
    "    tf.set_random_seed(1) # SEED\n",
    "    np.random.seed(1) # SEED\n",
    "    \n",
    "    dev_scores = [] ## SS - var used for early stopping\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    for i in range(50):\n",
    "        timestamp = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        shuffle(training_indices)\n",
    "        for j in range(train_size // BATCH_SIZE):\n",
    "            batch_indices = training_indices[j * BATCH_SIZE: (j + 1) * BATCH_SIZE] #JC\n",
    "            batch_feed_dict = {\n",
    "                story_pl: train_stories[batch_indices],\n",
    "                labels_pl: train_orders[batch_indices],\n",
    "                seq_len: seq_len_batch,\n",
    "                dropout_pl: global_dropout,\n",
    "                learn_rate: learn_rate_base * (learn_rate_discount ** i)\n",
    "            }\n",
    "            _, current_loss = sess.run([run_optimizer, loss], feed_dict=batch_feed_dict)\n",
    "            total_loss += current_loss\n",
    "            \n",
    "            if j in checkpoints:\n",
    "                dev_feed_dict = {\n",
    "                    story_pl: dev_stories,\n",
    "                    labels_pl: dev_orders,\n",
    "                    seq_len: seq_len_dev,\n",
    "                    dropout_pl: 1.0,\n",
    "                    learn_rate: learn_rate_base\n",
    "                }\n",
    "                dev_predictions = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "                dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predictions) * 100\n",
    "                dev_accuracies.append(dev_accuracy)\n",
    "                print(dev_accuracy)\n",
    "                if dev_accuracy == max(dev_accuracies):\n",
    "                    nn.save_model(sess)\n",
    "                \n",
    "                \n",
    "        train_feed_dict = {\n",
    "            story_pl: train_stories,\n",
    "            labels_pl: train_orders,\n",
    "            seq_len: seq_len_train,\n",
    "            dropout_pl: global_dropout,\n",
    "            learn_rate: learn_rate_base * (learn_rate_discount ** i)\n",
    "        }\n",
    "\n",
    "        dev_feed_dict = {\n",
    "            story_pl: dev_stories,\n",
    "            labels_pl: dev_orders,\n",
    "            seq_len: seq_len_dev,\n",
    "            dropout_pl: 1.0,\n",
    "            learn_rate: learn_rate_base\n",
    "        }\n",
    "\n",
    "        train_predictions = sess.run(predict, feed_dict=train_feed_dict)\n",
    "        train_accuracy = nn.calculate_accuracy(train_orders, train_predictions) * 100\n",
    "\n",
    "        dev_predictions = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "        dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predictions) * 100\n",
    "        dev_accuracies.append(dev_accuracy)\n",
    "        if dev_accuracy == max(dev_accuracies):\n",
    "            nn.save_model(sess)\n",
    "\n",
    "        print(\"Epoch: {0} | Loss: {1:>7,.0f} | Time: {2:.0f}s | Train acc: {3:.2f}% | Dev acc: {4:.2f}%\".\\\n",
    "              format(i+1,total_loss,time.time()-timestamp,train_accuracy,dev_accuracy))\n",
    "\n",
    "        ################### EARLY STOPPING [SS] ###################\n",
    "        if (early_stopping == True) and (i >= 1):\n",
    "            best = 0\n",
    "            if (dev_accuracy - max(dev_scores)) < -1:\n",
    "                print('Model suboptimal - NOT saving!')\n",
    "                print('')\n",
    "                print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX ')\n",
    "                print('Convergence Criterium met: Stopping Computation ')\n",
    "                print('')\n",
    "                print('Accuracy of Best Model / Saved Model on dev set: '+ str(round(max(dev_scores), 2))+'%')\n",
    "                break\n",
    "            else:\n",
    "                if dev_accuracy >= max(dev_scores):\n",
    "                    dev_scores.append(dev_accuracy)\n",
    "                    print('Best model so far - Saving!')\n",
    "                    nn.save_model(sess)\n",
    "                    # Best model will be saved and used in final assessment\n",
    "                else:\n",
    "                    dev_scores.append(dev_accuracy)\n",
    "                    print('Model suboptimal - NOT saving!')\n",
    "        else:\n",
    "            if i == 0:\n",
    "                print('Best model so far - Saving!')\n",
    "#                 nn.save_model(sess) # Always keeps best model saved\n",
    "            else:\n",
    "                print('Early Stopping set to \"False\" - Saving!')\n",
    "#                 nn.save_model(sess) # Always save current model if early_stopping deactivated\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.755730",
     "start_time": "2016-12-20T12:05:54.617471"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# LOAD TEST DATA\n",
    "data_test = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "test_stories, test_orders, _ = pipeline(data_test, vocab=vocab, max_sent_len_=max_sent_len)\n",
    "\n",
    "test_feed_dict = {story_pl: test_stories, labels_pl: test_orders, seq_len: np.full_like(np.arange(len(data_test)), max_sent_len), dropout_pl: 1, learn_rate: learn_rate_base}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:55.116609",
     "start_time": "2016-12-20T12:05:54.758571"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66830571886691614"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! ASSESSMENT 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "with tf.Session() as sess:\n",
    "    # LOAD THE MODEL\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './model/model.checkpoint')\n",
    "    \n",
    "    # RUN TEST SET EVALUATION\n",
    "    dev_predicted = sess.run(predict, feed_dict=test_feed_dict)\n",
    "    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "\n",
    "dev_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Appendix 1</font>: Pretrained Word Embeddings (using GloVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_len = 50\n",
    "\n",
    "glove = {}\n",
    "\n",
    "with open(\"glove.6B.\"+str(embedding_len)+\"d.txt\", 'r', encoding='utf-8') as f:    \n",
    "    for i, line in enumerate(f):\n",
    "        tokens = line.split(' ')\n",
    "        word = tokens[0]\n",
    "        entries = tokens[1:]\n",
    "        entries[-1] = entries[-1].strip()\n",
    "        glove[word] = [float(i) for i in entries]\n",
    "\n",
    "        \n",
    "embeddings_list = []\n",
    "\n",
    "embeddings_list.append([0] * embedding_len)\n",
    "embeddings_list.append([1] * embedding_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Appendix 2</font>: Kneser-Ney Language Model to eliminate OOVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class NGramKnesserNeyLM(object):\n",
    "    def __init__(self, train, missing_words, delta, order):\n",
    "\n",
    "        self.vocab = set(train)\n",
    "        self.order = order\n",
    "\n",
    "        self._delta = delta\n",
    "        self._word_identity = collections.defaultdict()\n",
    "        self._ngram_counts = collections.defaultdict()\n",
    "        self._history_counts = collections.defaultdict()\n",
    "        self._missing_words = missing_words\n",
    "\n",
    "        for i in range(1, len(train)):\n",
    "            all_histories = []\n",
    "            for ord in range(self.order,1,-1):\n",
    "                if tuple(train[i-ord+1:i]) != ():\n",
    "                    all_histories.append(tuple(train[i-ord+1:i]))\n",
    "\n",
    "            word = train[i]\n",
    "\n",
    "            for history in all_histories:\n",
    "\n",
    "                if len(history)+1 not in self._ngram_counts:\n",
    "                    self._ngram_counts[len(history)+1] = collections.defaultdict()\n",
    "                if (word, history) not in self._ngram_counts[len(history)+1]:\n",
    "                    self._ngram_counts[len(history)+1][(word, history)] = 1\n",
    "                else:\n",
    "                    self._ngram_counts[len(history)+1][(word, history)] += 1\n",
    "\n",
    "                if history not in self._history_counts:\n",
    "                    self._history_counts[history] = 1\n",
    "                else:\n",
    "                    self._history_counts[history] += 1\n",
    "\n",
    "                if history not in self._word_identity:\n",
    "                    self._word_identity[history] = {\"is_history\": set(), \"is_word\": set()}\n",
    "                self._word_identity[history]['is_history'] |= {word}\n",
    "                \n",
    "                if len(history) == 1:\n",
    "                    if word not in self._word_identity:\n",
    "                        self._word_identity[word] = {\"is_history\": set(), \"is_word\": set()}\n",
    "                    self._word_identity[word]['is_word'] |= {history}\n",
    "\n",
    "\n",
    "    def prob(self, prob_order, word, *history):\n",
    "\n",
    "        if prob_order == 1:\n",
    "            return len(self._word_identity[word][\"is_word\"]) / len(self._ngram_counts[2])\n",
    "\n",
    "        sub_history = tuple(history[-(prob_order-1):])\n",
    "        \n",
    "            \n",
    "        if sub_history not in self._history_counts:\n",
    "            if sub_history[0] in self._missing_words:\n",
    "                sub_history = list(sub_history)\n",
    "                sub_history[0] = lm.OOV\n",
    "                return self.prob( prob_order, word, *tuple(sub_history))\n",
    "            else:\n",
    "                return self.prob( prob_order - 1, word, *history)\n",
    "\n",
    "        if (word, sub_history) in self._ngram_counts[prob_order] and \\\n",
    "        self._ngram_counts[prob_order][(word, sub_history)] - self._delta > 0:\n",
    "            \n",
    "            left_hand_side = (self._ngram_counts[prob_order][(word, sub_history)] - self._delta) \\\n",
    "            / self._history_counts[sub_history]\n",
    "        else:\n",
    "            left_hand_side = 0\n",
    "\n",
    "        l = self._delta * len(self._word_identity[sub_history]['is_history']) \\\n",
    "        / self._history_counts[sub_history]\n",
    "\n",
    "        return left_hand_side + l * self.prob( prob_order - 1, word, *history)\n",
    "\n",
    "    \n",
    "    def probability(self, word, *history):\n",
    "\n",
    "        \n",
    "        if word in self.vocab:\n",
    "            return self.prob(self.order, word, *history)\n",
    "        elif word in self._missing_words:\n",
    "            return self.prob(self.order, lm.OOV, *history) / len(self._missing_words)\n",
    "        else:\n",
    "            return 0.0\n",
    "  \n",
    "OOV = '[OOV]'\n",
    "\n",
    "# Taken from statnlpbook.lm\n",
    "def inject_OOVs(data):\n",
    "    \"\"\"\n",
    "    Uses a heuristic to inject OOV symbols into a dataset.\n",
    "    Args:\n",
    "        data: the sequence of words to inject OOVs into.\n",
    "\n",
    "    Returns: the new sequence with OOV symbols injected.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for word in data:\n",
    "        if word in seen:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(OOV)\n",
    "            seen.add(word)\n",
    "    return result\n",
    "\n",
    "# Taken from statnlpbook.lm\n",
    "def sample(lm, init, amount):\n",
    "    \"\"\"\n",
    "    Sample from a language model.\n",
    "    Args:\n",
    "        lm: the language model\n",
    "        init: the initial sequence of words to condition on\n",
    "        amount: how long should the sampled sequence be\n",
    "    \"\"\"\n",
    "    words = list(lm.vocab)\n",
    "    result = []\n",
    "    result += init\n",
    "    for _ in range(0, amount):\n",
    "        history = result[-(lm.order - 1):]\n",
    "        probs = [lm.probability(word, *history) for word in words if word != \"\"]\n",
    "        sampled = np.random.choice(words, p=probs)\n",
    "        result.append(sampled)\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_lm(vocab, corpus):\n",
    "    \"\"\"\n",
    "    Return an instance of `lm.LanguageModel` defined over the given vocabulary.\n",
    "    Args:\n",
    "        vocab: the vocabulary the LM should be defined over. It is the union of the training and test words.\n",
    "    Returns:\n",
    "        a language model, instance of `lm.LanguageModel`.\n",
    "    \"\"\"\n",
    "\n",
    "    train_corpus = inject_OOVs(corpus)\n",
    "    kn3 = NGramKnesserNeyLM(train_corpus, vocab - set(train_corpus), 0.8971875, 3)\n",
    " \n",
    "    return kn3\n",
    "\n",
    "\n",
    "lm_train_corpus =  [tokenize(\"\".join(x[\"story\"])) for x in data_train[0:100]] \n",
    "lm_train_corpus = [y for x in lm_train_corpus for y in x]\n",
    "\n",
    "lm_vocab = set(lm_train_corpus)\n",
    "lm = create_lm(lm_vocab, lm_train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Appendix 3</font>: Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "stub_predicted = np.load('stub_orders.npy')\n",
    "MLP_predicted = np.load('MLP_orders.npy')\n",
    "BiRNN_predicted = np.load('Bi-RNN_oders.npy')\n",
    "dev_orders = np.array([dev['order'] for dev in data_dev])\n",
    "\n",
    "models_EA = [stub_predicted, MLP_predicted, BiRNN_predicted]\n",
    "models_EA_flat = [list(array.flatten()) for array in models_EA]\n",
    "model_names = ['Stub', 'Multi-layer Perceptron', 'Bi-RNN']\n",
    "dev_orders_cm = dev_orders.flatten()\n",
    "\n",
    "color = sns.color_palette(\"Set2\", 10)\n",
    "\n",
    "temp_max = []\n",
    "temp_diag = []\n",
    "\n",
    "for i, model in enumerate(models_EA_flat):\n",
    "    temp_max.append(np.max(confusion_matrix(dev_orders_cm, model)))\n",
    "    temp_array = confusion_matrix(dev_orders_cm, model)\n",
    "    np.fill_diagonal(temp_array, 0)\n",
    "    temp_diag.append(np.max(temp_array))\n",
    "    \n",
    "max_left = max(temp_max)\n",
    "max_right = max(temp_diag)\n",
    "\n",
    "num_models = len(models_EA)\n",
    "\n",
    "fig = plt.figure(figsize=[12, 3*num_models])\n",
    "ax = [None] * num_models * 2\n",
    "\n",
    "for i, model in enumerate(models_EA_flat):\n",
    "    ax[2*i] = fig.add_subplot(num_models, 2, 2*i+1)\n",
    "    cm_values = confusion_matrix(dev_orders_cm, model)\n",
    "    sns.heatmap(cm_values, cmap=\"Blues\", vmax=max_left, vmin=0)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel(model_names[i], fontsize=20)\n",
    "    ax[2*i].xaxis.set_ticks_position(\"top\")\n",
    "    if not i:\n",
    "        plt.title('Confusion matrix with diagonal\\n', fontsize=20)\n",
    "\n",
    "    ax[2*i] = fig.add_subplot(num_models, 2, 2*i+2)\n",
    "    cm_values_diag = np.empty_like(cm_values)\n",
    "    cm_values_diag[:] = cm_values\n",
    "    np.fill_diagonal(cm_values_diag, 0)\n",
    "    sns.heatmap(cm_values_diag, cmap=\"Blues\", vmax=max_right, vmin=0)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('Actual class')\n",
    "    ax[2*i].xaxis.set_ticks_position(\"top\")\n",
    "    if not i:\n",
    "        plt.title(\"Confusion matrix without diagonal\\n\", fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# params for plotting\n",
    "even = num_models % 2 == 0\n",
    "width = 0.8/num_models\n",
    "if even:\n",
    "    offset = [(n*width) - (0.4-1/(5*(num_models/2))) for n in range(num_models)]\n",
    "if not even:\n",
    "    offset = [(n-(num_models-1)/2)*width for n in range(num_models)]\n",
    "    \n",
    "fig = plt.figure(figsize=[8+(num_models*2),6])\n",
    "counter_rep = np.array([[0]*5] * num_models)\n",
    "\n",
    "for i, model in enumerate(models_EA):\n",
    "    for pred_story in model:\n",
    "        freqs = [len([pred for pred in pred_story if pred == n]) for n in range(5)]\n",
    "        counter_rep[i][max(freqs)-1] += 1\n",
    "    plt.bar(np.array(range(1,6))+offset[i], counter_rep[i], width=width, align='center', color=color[i]) \n",
    "    \n",
    "plt.title(\"Max frequency of individual labels per story\")\n",
    "plt.xticks(range(1,6))\n",
    "plt.legend(model_names)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=[8+(num_models*2),6])\n",
    "counter_double = np.array([[0]*5] * num_models)\n",
    "for i, model in enumerate(models_EA):\n",
    "    for pred_story in model:\n",
    "        freqs = [len([pred for pred in pred_story if pred == n]) for n in range(5)]\n",
    "        truth_array = np.array([2]*5)\n",
    "        indices = [i for i, n in enumerate(freqs) if n == 2]\n",
    "        counter_double[i][indices] += 1\n",
    "    plt.bar(np.array(range(5))+offset[i], counter_double[i], width=width, align='center', color=color[i])\n",
    "    \n",
    "plt.title(\"Double labellings by predicted label\")\n",
    "plt.xticks(range(5))\n",
    "plt.legend(model_names)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=[8+(num_models*2),6])\n",
    "counter_prec = np.array([[0]*5] * num_models)\n",
    "for i, model in enumerate(models_EA):\n",
    "    for j, pred_story in enumerate(model):\n",
    "        truth_array = np.array(range(5))\n",
    "        freqs = np.array([sum(pred_story == dev_orders[j]) == n for n in range(5)])\n",
    "        num_freqs = freqs.astype(int)\n",
    "        counter_prec[i] += num_freqs\n",
    "    plt.bar(np.array(range(1,6))+offset[i], counter_prec[i], width=width, align='center', color=color[i])\n",
    "    \n",
    "plt.title(\"Distribution of number of correct guesses per story\")\n",
    "plt.xticks(range(1,6))\n",
    "plt.legend(model_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Appendix 4</font>: HyperParameter Tuning Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "_snlp_book_dir = \"./\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "import statnlpbook.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from random import shuffle\n",
    "from random import seed\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "\n",
    "def import_glove():\n",
    "\n",
    "    print(\"Importing Glove embeddings\")\n",
    "    with open(\"log.txt\", \"a+\") as f:\n",
    "                f.write(datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\") + \"\\tImporting Glove embeddings\\n\")\n",
    "\n",
    "    with open(\"glove.6B.\"+str(embedding_len)+\"d.txt\", 'r', encoding='utf-8') as f:    \n",
    "        for i, line in enumerate(f):\n",
    "            tokens = line.split(' ')\n",
    "            word = tokens[0]\n",
    "            entries = tokens[1:]\n",
    "            entries[-1] = entries[-1].strip()\n",
    "            glove[word] = [float(i) for i in entries]\n",
    "\n",
    "\n",
    "    embeddings_list.append([0] * embedding_len)\n",
    "    embeddings_list.append([1] * embedding_len)\n",
    "\n",
    "import sys\n",
    "\n",
    "class PorterStemmer:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The main part of the stemming algorithm starts here.\n",
    "        b is a buffer holding a word to be stemmed. The letters are in b[k0],\n",
    "        b[k0+1] ... ending at b[k]. In fact k0 = 0 in this demo program. k is\n",
    "        readjusted downwards as the stemming progresses. Zero termination is\n",
    "        not in fact used in the algorithm.\n",
    "\n",
    "        Note that only lower case sequences are stemmed. Forcing to lower case\n",
    "        should be done before stem(...) is called.\n",
    "        \"\"\"\n",
    "\n",
    "        self.b = \"\"  # buffer for word to be stemmed\n",
    "        self.k = 0\n",
    "        self.k0 = 0\n",
    "        self.j = 0   # j is a general offset into the string\n",
    "\n",
    "    def cons(self, i):\n",
    "        \"\"\"cons(i) is TRUE <=> b[i] is a consonant.\"\"\"\n",
    "        if self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or self.b[i] == 'o' or self.b[i] == 'u':\n",
    "            return 0\n",
    "        if self.b[i] == 'y':\n",
    "            if i == self.k0:\n",
    "                return 1\n",
    "            else:\n",
    "                return (not self.cons(i - 1))\n",
    "        return 1\n",
    "\n",
    "    def m(self):\n",
    "        \"\"\"m() measures the number of consonant sequences between k0 and j.\n",
    "        if c is a consonant sequence and v a vowel sequence, and <..>\n",
    "        indicates arbitrary presence,\n",
    "\n",
    "           <c><v>       gives 0\n",
    "           <c>vc<v>     gives 1\n",
    "           <c>vcvc<v>   gives 2\n",
    "           <c>vcvcvc<v> gives 3\n",
    "           ....\n",
    "        \"\"\"\n",
    "        n = 0\n",
    "        i = self.k0\n",
    "        while 1:\n",
    "            if i > self.j:\n",
    "                return n\n",
    "            if not self.cons(i):\n",
    "                break\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "        while 1:\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "            n = n + 1\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if not self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "\n",
    "    def vowelinstem(self):\n",
    "        \"\"\"vowelinstem() is TRUE <=> k0,...j contains a vowel\"\"\"\n",
    "        for i in range(self.k0, self.j + 1):\n",
    "            if not self.cons(i):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def doublec(self, j):\n",
    "        \"\"\"doublec(j) is TRUE <=> j,(j-1) contain a double consonant.\"\"\"\n",
    "        if j < (self.k0 + 1):\n",
    "            return 0\n",
    "        if (self.b[j] != self.b[j-1]):\n",
    "            return 0\n",
    "        return self.cons(j)\n",
    "\n",
    "    def cvc(self, i):\n",
    "        \"\"\"cvc(i) is TRUE <=> i-2,i-1,i has the form consonant - vowel - consonant\n",
    "        and also if the second c is not w,x or y. this is used when trying to\n",
    "        restore an e at the end of a short  e.g.\n",
    "\n",
    "           cav(e), lov(e), hop(e), crim(e), but\n",
    "           snow, box, tray.\n",
    "        \"\"\"\n",
    "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
    "            return 0\n",
    "        ch = self.b[i]\n",
    "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def ends(self, s):\n",
    "        \"\"\"ends(s) is TRUE <=> k0,...k ends with the string s.\"\"\"\n",
    "        length = len(s)\n",
    "        if s[length - 1] != self.b[self.k]: # tiny speed-up\n",
    "            return 0\n",
    "        if length > (self.k - self.k0 + 1):\n",
    "            return 0\n",
    "        if self.b[self.k-length+1:self.k+1] != s:\n",
    "            return 0\n",
    "        self.j = self.k - length\n",
    "        return 1\n",
    "\n",
    "    def setto(self, s):\n",
    "        \"\"\"setto(s) sets (j+1),...k to the characters in the string s, readjusting k.\"\"\"\n",
    "        length = len(s)\n",
    "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
    "        self.k = self.j + length\n",
    "\n",
    "    def r(self, s):\n",
    "        \"\"\"r(s) is used further down.\"\"\"\n",
    "        if self.m() > 0:\n",
    "            self.setto(s)\n",
    "\n",
    "    def step1ab(self):\n",
    "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
    "\n",
    "           caresses  ->  caress\n",
    "           ponies    ->  poni\n",
    "           ties      ->  ti\n",
    "           caress    ->  caress\n",
    "           cats      ->  cat\n",
    "\n",
    "           feed      ->  feed\n",
    "           agreed    ->  agree\n",
    "           disabled  ->  disable\n",
    "\n",
    "           matting   ->  mat\n",
    "           mating    ->  mate\n",
    "           meeting   ->  meet\n",
    "           milling   ->  mill\n",
    "           messing   ->  mess\n",
    "\n",
    "           meetings  ->  meet\n",
    "        \"\"\"\n",
    "        if self.b[self.k] == 's':\n",
    "            if self.ends(\"sses\"):\n",
    "                self.k = self.k - 2\n",
    "            elif self.ends(\"ies\"):\n",
    "                self.setto(\"i\")\n",
    "            elif self.b[self.k - 1] != 's':\n",
    "                self.k = self.k - 1\n",
    "        if self.ends(\"eed\"):\n",
    "            if self.m() > 0:\n",
    "                self.k = self.k - 1\n",
    "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
    "            self.k = self.j\n",
    "            if self.ends(\"at\"):   self.setto(\"ate\")\n",
    "            elif self.ends(\"bl\"): self.setto(\"ble\")\n",
    "            elif self.ends(\"iz\"): self.setto(\"ize\")\n",
    "            elif self.doublec(self.k):\n",
    "                self.k = self.k - 1\n",
    "                ch = self.b[self.k]\n",
    "                if ch == 'l' or ch == 's' or ch == 'z':\n",
    "                    self.k = self.k + 1\n",
    "            elif (self.m() == 1 and self.cvc(self.k)):\n",
    "                self.setto(\"e\")\n",
    "\n",
    "    def step1c(self):\n",
    "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
    "        if (self.ends(\"y\") and self.vowelinstem()):\n",
    "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
    "\n",
    "    def step2(self):\n",
    "        \"\"\"step2() maps double suffices to single ones.\n",
    "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
    "        string before the suffix must give m() > 0.\n",
    "        \"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
    "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"bli\"):       self.r(\"ble\") # --DEPARTURE--\n",
    "            # To match the published algorithm, replace this phrase with\n",
    "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
    "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
    "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
    "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
    "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
    "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"alism\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
    "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
    "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
    "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
    "        elif self.b[self.k - 1] == 'g': # --DEPARTURE--\n",
    "            if self.ends(\"logi\"):      self.r(\"log\")\n",
    "        # To match the published algorithm, delete this phrase\n",
    "\n",
    "    def step3(self):\n",
    "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
    "        if self.b[self.k] == 'e':\n",
    "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
    "            elif self.ends(\"ative\"):   self.r(\"\")\n",
    "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
    "        elif self.b[self.k] == 'i':\n",
    "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
    "        elif self.b[self.k] == 'l':\n",
    "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
    "            elif self.ends(\"ful\"):     self.r(\"\")\n",
    "        elif self.b[self.k] == 's':\n",
    "            if self.ends(\"ness\"):      self.r(\"\")\n",
    "\n",
    "    def step4(self):\n",
    "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"al\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"ance\"): pass\n",
    "            elif self.ends(\"ence\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"er\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'i':\n",
    "            if self.ends(\"ic\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"able\"): pass\n",
    "            elif self.ends(\"ible\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'n':\n",
    "            if self.ends(\"ant\"): pass\n",
    "            elif self.ends(\"ement\"): pass\n",
    "            elif self.ends(\"ment\"): pass\n",
    "            elif self.ends(\"ent\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
    "            elif self.ends(\"ou\"): pass\n",
    "            # takes care of -ous\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"ism\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"ate\"): pass\n",
    "            elif self.ends(\"iti\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'u':\n",
    "            if self.ends(\"ous\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'v':\n",
    "            if self.ends(\"ive\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'z':\n",
    "            if self.ends(\"ize\"): pass\n",
    "            else: return\n",
    "        else:\n",
    "            return\n",
    "        if self.m() > 1:\n",
    "            self.k = self.j\n",
    "\n",
    "    def step5(self):\n",
    "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
    "        m() > 1.\n",
    "        \"\"\"\n",
    "        self.j = self.k\n",
    "        if self.b[self.k] == 'e':\n",
    "            a = self.m()\n",
    "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
    "                self.k = self.k - 1\n",
    "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
    "            self.k = self.k -1\n",
    "\n",
    "    def stem(self, p, i, j):\n",
    "        \"\"\"In stem(p,i,j), p is a char pointer, and the string to be stemmed\n",
    "        is from p[i] to p[j] inclusive. Typically i is zero and j is the\n",
    "        offset to the last character of a string, (p[j+1] == '\\0'). The\n",
    "        stemmer adjusts the characters p[i] ... p[j] and returns the new\n",
    "        end-point of the string, k. Stemming never increases word length, so\n",
    "        i <= k <= j. To turn the stemmer into a module, declare 'stem' as\n",
    "        extern, and delete the remainder of this file.\n",
    "        \"\"\"\n",
    "        # copy the parameters into statics\n",
    "        self.b = p\n",
    "        self.k = j\n",
    "        self.k0 = i\n",
    "        if self.k <= self.k0 + 1:\n",
    "            return self.b # --DEPARTURE--\n",
    "\n",
    "        # With this line, strings of length 1 or 2 don't go through the\n",
    "        # stemming process, although no mention is made of this in the\n",
    "        # published algorithm. Remove the line to match the published\n",
    "        # algorithm.\n",
    "\n",
    "        self.step1ab()\n",
    "        self.step1c()\n",
    "        self.step2()\n",
    "        self.step3()\n",
    "        self.step4()\n",
    "        self.step5()\n",
    "        return self.b[self.k0:self.k+1]\n",
    "\n",
    "use_stemmer = True\n",
    "\n",
    "if use_stemmer == True:\n",
    "    pstem = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize(input):  # [SS]\n",
    "    b = split(input)\n",
    "    c = replace(b)\n",
    "    d = strip_numbers(c)\n",
    "    e = remove_short(d)\n",
    "    # f = remove_stop(e)\n",
    "    g = tolower(d)\n",
    "    if use_stemmer == True:\n",
    "        h = stemit(g, pstem)\n",
    "        return h\n",
    "    else:\n",
    "        return g\n",
    "\n",
    "\n",
    "def split(input, excl=False):\n",
    "    a = input.split(' ')\n",
    "\n",
    "    a_new = []\n",
    "    for word in a:\n",
    "\n",
    "        if ('n\\'t') in word:\n",
    "            if ('won\\'t') in word:\n",
    "                b = 'will'\n",
    "                a_new.append(b)\n",
    "                a_new.append('not')\n",
    "            else:\n",
    "                b = word[:-3]\n",
    "                a_new.append(b)\n",
    "                a_new.append('not')\n",
    "        elif ('\\'ll') in word:\n",
    "            b = word[:-3]\n",
    "            a_new.append(b)\n",
    "            a_new.append('will')\n",
    "        elif ('\\'m') in word:\n",
    "            b = word[:-2]\n",
    "            a_new.append(b)\n",
    "            a_new.append('am')\n",
    "        elif ('\\'d') in word:\n",
    "            b = word[:-2]\n",
    "            a_new.append(b)\n",
    "            a_new.append('would')\n",
    "        elif ('\\'s') in word:\n",
    "            b = word[:-2]\n",
    "            a_new.append(b)\n",
    "            a_new.append('\\'s')\n",
    "        elif ('\\'re') in word:\n",
    "            b = word[:-3]\n",
    "            a_new.append(b)\n",
    "            a_new.append('are')\n",
    "        else:\n",
    "            a_new.append(word)\n",
    "\n",
    "    sent_string = \" \".join(str(x) for x in a_new)\n",
    "    # print(a_new)\n",
    "    b = re.split(r'[`\\-=~@#$%^&*()_+\\[\\]{};\\\\:\"|<,/<>\\s]', sent_string)\n",
    "    # print(b)\n",
    "\n",
    "    sent = []\n",
    "    for word in b:\n",
    "        if (word != ' ') and (word != ''):\n",
    "            if word.endswith('?'):\n",
    "                b = word[:-1]\n",
    "                sent.append(b)\n",
    "                # sent.append('?')\n",
    "            elif word.endswith('!'):\n",
    "                b = word[:-1]\n",
    "                sent.append(b)\n",
    "                # sent.append('!')\n",
    "            elif word.endswith('.'):\n",
    "                b = word[:-1]\n",
    "                sent.append(b)\n",
    "                # sent.append('.')\n",
    "            else:\n",
    "                sent.append(word)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def tolower(input):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        sent.append(word.lower())\n",
    "    return sent\n",
    "\n",
    "\n",
    "def remove_stop(input):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        if word not in stop_words.ENGLISH_STOP_WORDS:\n",
    "            sent.append(word)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def replace(input):\n",
    "    sent = []\n",
    "\n",
    "    for word in input:\n",
    "        a = ''.join(e for e in word if e.isalnum())\n",
    "        sent.append(a)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def strip_numbers(input):\n",
    "    sent = []\n",
    "\n",
    "    for word in input:\n",
    "        if bool(re.search(r'\\d', word)) == False:\n",
    "            sent.append(word)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def remove_short(input):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        if len(word) > 1:\n",
    "            sent.append(word)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def stemit(input, stemmer):\n",
    "    sent = []\n",
    "    for word in input:\n",
    "        sent.append(pstem.stem(word, 0, len(word) - 1))\n",
    "    return sent\n",
    "\n",
    "\n",
    "def pipeline(data, vocab=None, max_sent_len_=None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "        \n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_orders = []\n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        for sentence in instance['story']:\n",
    "            sent = []\n",
    "            tokenized = tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                if not is_ext_vocab and token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                    if token in glove:\n",
    "                        embeddings_list.append(glove[token])\n",
    "                    else:\n",
    "                        embeddings_list.append([1] * embedding_len)\n",
    "                if token not in vocab:\n",
    "                    token_id = vocab['<OOV>']\n",
    "                else:\n",
    "                    token_id = vocab[token]\n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "        data_sentences.append(sents)\n",
    "        data_orders.append(instance['order'])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "    \n",
    "    return out_sentences, out_orders, vocab\n",
    "\n",
    "    \n",
    "\n",
    "def preprocess_datasets():\n",
    "    train_stories, train_orders, vocab = pipeline(data_train)\n",
    "    max_sent_len = train_stories.shape[2]\n",
    "    dev_stories, dev_orders, _ = pipeline(data_dev, vocab=vocab, max_sent_len_=max_sent_len)\n",
    "    return [train_stories, train_orders, vocab, max_sent_len, dev_stories, dev_orders]\n",
    "\n",
    "\n",
    "## SS - Added whole cell for implementing different optimizers and gradient clipping\n",
    "\n",
    "def optimize(loss, clip_low, clip_high, learn_rate, optimizer, grad_clipping=False, momentum=1):\n",
    "    \n",
    "    # Optimizer selection\n",
    "    if optimizer=='adam':\n",
    "        opt_op = tf.train.AdamOptimizer(learn_rate)\n",
    "    elif optimizer=='adagrad':\n",
    "        opt_op = tf.train.AdagradOptimizer(learn_rate)\n",
    "    elif optimizer=='adadelta':\n",
    "        opt_op = tf.train.AdadeltaOptimizer(learn_rate)\n",
    "    elif optimizer=='momentum':\n",
    "        print('Momentum currently set to '+str(momentum))\n",
    "        opt_op = tf.train.MomentumOptimizer(learning_rate=learn_rate, momentum=momentum)\n",
    "    \n",
    "    if grad_clipping == True:\n",
    "        # Gradient Clipping [SS]\n",
    "        gvs = opt_op.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, clip_low, clip_high), var) for grad, var in gvs]\n",
    "        return opt_op.apply_gradients(capped_gvs)\n",
    "    else:\n",
    "        return opt_op.minimize(loss)\n",
    "\n",
    "        # Model architecture\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(input_size, hidden_size, num_stacked_rnns, global_dropout, reg_lmbda, activation_ff, use_pre_trained_embeddings, optimizer, learn_rate_base, grad_clipping, clip_high, clip_low):\n",
    "    dev_accuracies = []\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    tf.set_random_seed(1) # SEED\n",
    "    np.random.seed(1) # SEED\n",
    "\n",
    "    # Bidirectional RNN sequence lengths\n",
    "    seq_len_batch = np.full_like(np.arange(BATCH_SIZE), max_sent_len)\n",
    "    seq_len_train = np.full_like(np.arange(len(data_train)), max_sent_len)\n",
    "    seq_len_dev = np.full_like(np.arange(len(data_dev)), max_sent_len)\n",
    "\n",
    "    # Converting Embeddings to NP Array\n",
    "\n",
    "\n",
    "    target_size = 5 # num classes\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    train_size = len(data_train) #JC\n",
    "    training_indices = list(range(train_size)) #JC\n",
    "\n",
    "    ## Placeholders\n",
    "    dropout_pl = tf.placeholder(tf.float32, shape=(None))\n",
    "    story_pl = tf.placeholder(tf.int64, [None, None, None], \"story\")\n",
    "    labels_pl = tf.placeholder(tf.int64, [None, None], \"labels\")\n",
    "    seq_len = tf.placeholder(tf.int64, [None])\n",
    "    learn_rate = tf.placeholder(tf.float32, None)\n",
    "    batch_size = tf.shape(story_pl)[0]\n",
    "\n",
    "    # Word Embeddings\n",
    "    if use_pre_trained_embeddings:\n",
    "        W = tf.get_variable(name=\"W\", shape=embeddings_list.shape, initializer=tf.constant_initializer(embeddings_list), trainable=False)\n",
    "    else:\n",
    "        W = tf.get_variable(\"W\", [vocab_size, input_size], initializer=tf.random_uniform_initializer(-1, 1))\n",
    "        \n",
    "    sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story_pl)]\n",
    "    story_embedded = [tf.nn.embedding_lookup(W, sent) for sent in sentences]\n",
    "\n",
    "    with tf.variable_scope(\"encoder\") as varscope:\n",
    "        tf.set_random_seed(1) # SEED\n",
    "        np.random.seed(1) # SEED\n",
    "        \n",
    "        # Bidirectional Multi-Layer RNN with dropout\n",
    "        cell_fw = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "        cell_bw = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "        cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, input_keep_prob=global_dropout, output_keep_prob=global_dropout) # [SS]\n",
    "        cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, input_keep_prob=global_dropout, output_keep_prob=global_dropout) # [SS]\n",
    "        # SS - introduced stacked RNN\n",
    "        cell_fw = tf.nn.rnn_cell.MultiRNNCell([cell_fw] * num_stacked_rnns, state_is_tuple=True)\n",
    "        cell_bw = tf.nn.rnn_cell.MultiRNNCell([cell_bw] * num_stacked_rnns, state_is_tuple=True)\n",
    "        cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, output_keep_prob=global_dropout)\n",
    "        cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, output_keep_prob=global_dropout)\n",
    "            \n",
    "        sen_final_state = [None] * 5\n",
    "        sen_h = [None] * 5\n",
    "        outputs = [None] * 5\n",
    "        sen_h_fw = [None] * 5\n",
    "        sen_h_bw = [None] * 5\n",
    "        \n",
    "        for i in range(target_size):\n",
    "            _,sen_final_state[i] =\\\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=story_embedded[i],\n",
    "                                            dtype=tf.float32, sequence_length=seq_len)         \n",
    "            sen_h_fw[i], sen_h_bw[i] = sen_final_state[i]\n",
    "            sen_h_fw[i] = sen_h_fw[i][-1].h            \n",
    "            varscope.reuse_variables()\n",
    "\n",
    "    h_fw = tf.concat(1, sen_h_fw)\n",
    "\n",
    "\n",
    "    relu_layer = tf.contrib.layers.fully_connected(h_fw, 5 * input_size, activation_fn=activation_ff)\n",
    "    logits_flat_fw = tf.contrib.layers.fully_connected(relu_layer, 5 * target_size, activation_fn=None)    # [batch_size x 5*target_size]\n",
    "\n",
    "    logits = tf.reshape((logits_flat_fw), [-1, 5, target_size])   \n",
    "\n",
    "    tf_vars = tf.trainable_variables() \n",
    "    l2_loss = tf.mul(tf.add_n([tf.nn.l2_loss(v) for v in tf_vars if 'bias' not in v.name ]), reg_lmbda)\n",
    "    unreg_loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_pl))\n",
    "    loss = tf.add(unreg_loss, l2_loss, name='loss')\n",
    "\n",
    "    run_optimizer = optimize(loss, clip_low, clip_high, learn_rate, optimizer, grad_clipping) # [SS]\n",
    "\n",
    "    unpacked_logits = [tensor for tensor in tf.unpack(logits, axis=1)]\n",
    "    softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "    softmaxed_logits = tf.pack(softmaxes, axis=1)\n",
    "    predict = tf.arg_max(softmaxed_logits, 2)\n",
    "\n",
    "    learn_rate_discount = random.uniform(0.9, 1.0)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.set_random_seed(1) \n",
    "        np.random.seed(1) \n",
    "        \n",
    "        dev_scores = [] \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        with open(\"log.txt\", \"a+\") as f:\n",
    "            f.write(\"-----------------------------------------------------------------------\\n\")\n",
    "            f.write(datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\") + \"\\tStarting Training with:\\n\")\n",
    "            f.write(\"\\t input_size = \" +str(input_size)+\"\\n\")\n",
    "            f.write(\"\\t hidden_size = \" + str(hidden_size)+\"\\n\")\n",
    "            f.write(\"\\t num_stacked_rnns = \" + str(num_stacked_rnns)+\"\\n\")\n",
    "            f.write(\"\\t global_dropout = \" + str(global_dropout)+\"\\n\")\n",
    "            f.write(\"\\t reg_lmbda = \" + str(reg_lmbda)+\"\\n\")\n",
    "            f.write(\"\\t activation_ff = \" +str(activation_ff )+\"\\n\")\n",
    "            f.write(\"\\t use_pre_trained_embeddings = \" +str(use_pre_trained_embeddings )+\"\\n\")\n",
    "            f.write(\"\\t optimizer = \" + str(optimizer)+\"\\n\")\n",
    "            f.write(\"\\t learn_rate_base = \" + str(learn_rate_base)+\"\\n\")\n",
    "            f.write(\"\\t learn_rate_discount = \" + str(learn_rate_discount) + \"\\n\")\n",
    "            f.write(\"\\t grad_clipping = \" + str(grad_clipping)+\"\\n\")\n",
    "            f.write(\"\\t clip_high = \" + str(clip_high)+\"\\n\")\n",
    "            f.write(\"\\t clip_low = \" + str(clip_low)+\"\\n\")\n",
    "            f.write(\"\\t stemmer = \" + str(use_stemmer)+\"\\n\")\n",
    "            f.write(\"\\t batch_size = \" + str(BATCH_SIZE)+\"\\n\")\n",
    "        for i in range(25):\n",
    "            timestamp = time.time()\n",
    "            total_loss = 0\n",
    "\n",
    "            shuffle(training_indices)\n",
    "            for j in range(train_size // BATCH_SIZE):\n",
    "                batch_indices = training_indices[j * BATCH_SIZE: (j + 1) * BATCH_SIZE]\n",
    "                batch_feed_dict = {\n",
    "                    story_pl: train_stories[batch_indices],\n",
    "                    labels_pl: train_orders[batch_indices],\n",
    "                    seq_len: seq_len_batch,\n",
    "                    dropout_pl: global_dropout,\n",
    "                    learn_rate: learn_rate_base * (learn_rate_discount ** i)\n",
    "                }\n",
    "                _, current_loss = sess.run([run_optimizer, loss], feed_dict=batch_feed_dict)\n",
    "                total_loss += current_loss\n",
    "                \n",
    "                if j in checkpoints:\n",
    "                    dev_feed_dict = {\n",
    "                        story_pl: dev_stories,\n",
    "                        labels_pl: dev_orders,\n",
    "                        seq_len: seq_len_dev,\n",
    "                        dropout_pl: 1.0,\n",
    "                        learn_rate: learn_rate_base\n",
    "                      }\n",
    "                    dev_guesses = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "                    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_guesses) * 100\n",
    "                    dev_accuracies.append(dev_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "            train_feed_dict = {\n",
    "                story_pl: train_stories,\n",
    "                labels_pl: train_orders,\n",
    "                seq_len: seq_len_train,\n",
    "                dropout_pl: global_dropout,\n",
    "                learn_rate: learn_rate_base * (learn_rate_discount ** i)\n",
    "            }\n",
    "\n",
    "            dev_feed_dict = {\n",
    "                story_pl: dev_stories,\n",
    "                labels_pl: dev_orders,\n",
    "                seq_len: seq_len_dev,\n",
    "                dropout_pl: 1.0,\n",
    "                learn_rate: learn_rate_base\n",
    "            }\n",
    "\n",
    "            train_predictions = sess.run(predict, feed_dict=train_feed_dict)\n",
    "            train_accuracy = nn.calculate_accuracy(train_orders, train_predictions) * 100\n",
    "\n",
    "            dev_predictions = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "            dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predictions) * 100\n",
    "            dev_accuracies.append(dev_accuracy)\n",
    "\n",
    "            print(\"Epoch: {0} | Loss: {1:>7,.0f} | Time: {2:.0f}s | Train acc: {3:.2f}% | Dev acc: {4:.2f}%\".\\\n",
    "                  format(i+1,total_loss,time.time()-timestamp,train_accuracy,dev_accuracy))\n",
    "            with open(\"log.txt\", \"a+\") as f:\n",
    "                f.write(datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\") + \"\\t\" + \"Epoch: {0} | Loss: {1:>7,.0f} | Time: {2:.0f}s | Train acc: {3:.2f}% | Dev acc: {4:.2f}% \\n\".\\\n",
    "                  format(i+1,total_loss,time.time()-timestamp,train_accuracy,dev_accuracy))\n",
    "\n",
    "            if (early_stopping == True) and (i >= 1):\n",
    "                best = 0\n",
    "                if (dev_accuracy - max(dev_scores)) < - 1.5:\n",
    "                    print('Model suboptimal - NOT saving!')\n",
    "                    print('')\n",
    "                    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX ')\n",
    "                    print('Convergence Criterium met: Stopping Computation ')\n",
    "                    print('')\n",
    "                    print('Accuracy of Best Model / Saved Model on dev set: '+ str(round(max(dev_scores), 2))+'%')\n",
    "                    break\n",
    "                else:\n",
    "                    if dev_accuracy >= max(dev_scores):\n",
    "                        dev_scores.append(dev_accuracy)\n",
    "                        print('Best model so far - Saving!')\n",
    "                        nn.save_model(sess)\n",
    "                        # Best model will be saved and used in final assessment\n",
    "                    else:\n",
    "                        dev_scores.append(dev_accuracy)\n",
    "                        print('Model suboptimal - NOT saving!')\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    print('Best model so far - Saving!')\n",
    "                    nn.save_model(sess) # Always keeps best model saved\n",
    "                else:\n",
    "                    print('Early Stopping set to \"False\" - Saving!')\n",
    "                    nn.save_model(sess) # Always save current model if early_stopping deactivated\n",
    "                dev_scores.append(dev_accuracy)\n",
    "            \n",
    "\n",
    "    moving_average = [np.mean(dev_accuracies[n-5:n+6]) for n in range(5, len(dev_accuracies)-6)]\n",
    "\n",
    "    optimal_epoch = 0.6 + (moving_average.index(max(moving_average))/10)\n",
    "    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + '.npy'\n",
    "\n",
    "    with open(\"log.txt\", \"a+\") as f:\n",
    "        f.write('Best avg accuracy: {0:.2f}%, with optimal epoch: {1:.1f}\\n'.format(max(moving_average), optimal_epoch))\n",
    "    np.save(timestamp, dev_accuracies)\n",
    "    \n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "binary_list = [True,False]\n",
    "embeddings_list_sz = [50, 100, 200, 300]\n",
    "\n",
    "while(True):\n",
    "\n",
    "    use_pre_trained_embeddings = False\n",
    "    if use_pre_trained_embeddings:\n",
    "        input_size = embeddings_list_sz[random.randint(0,len(embeddings_list_sz)-1)]\n",
    "    else:\n",
    "        input_size = random.randint(10,50)\n",
    "    embedding_len = input_size\n",
    "    hidden_size = random.randint(30, 100) #\n",
    "    num_stacked_rnns = random.randint(2,5) # [SS] - Specify number of stacked RNNs\n",
    "    BATCH_SIZE = int(2 ** random.uniform(6,8))\n",
    "    global_dropout = 1 # [SS] - specify dropout KEEP probability\n",
    "    reg_lmbda = random.uniform(0, 0.025) # [SS] - l2 regularization lambda\n",
    "    activation_ff = tf.nn.relu\n",
    "    checkpoints = [int(n/10*(len(data_train)/BATCH_SIZE)) for n in range(1,10)]\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = 'adam'\n",
    "    learn_rate_base = 10 ** (-1 * random.randint(2, 4))\n",
    "    # Gradient Clipping\n",
    "    grad_clipping = random.choice([True, False])\n",
    "    clip_high = random.uniform(.5, 2)\n",
    "    clip_low = -1*clip_high\n",
    "    early_stopping = False\n",
    "    train_stories = None\n",
    "    train_orders = None\n",
    "    max_sent_len = None\n",
    "    dev_orders = None\n",
    "    dev_stories = None\n",
    "    glove = {}\n",
    "    embeddings_list = []\n",
    "\n",
    "    if use_pre_trained_embeddings:\n",
    "        import_glove()            \n",
    "\n",
    "    [train_stories, train_orders, vocab, max_sent_len, dev_stories, dev_orders] = preprocess_datasets()\n",
    "    embeddings_list = np.asarray(embeddings_list)\n",
    "    train(input_size, hidden_size, num_stacked_rnns, global_dropout, reg_lmbda, activation_ff, use_pre_trained_embeddings, optimizer, learn_rate_base, grad_clipping, clip_high, clip_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Appendix 5</font>: Moving Average to remove noise in `dev_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "dev_accuracies = np.load('2017-02-05 09-25-05.npy')\n",
    "\n",
    "fig = plt.figure(figsize=[10,5])\n",
    "moving_average = [np.mean(dev_accuracies[n-5:n+6]) for n in range(5, len(dev_accuracies)-6)]\n",
    "plt.plot(np.linspace(0,len(dev_accuracies)/10,len(dev_accuracies)),dev_accuracies, linewidth=0.5)\n",
    "plt.plot(np.linspace(0.5,len(dev_accuracies)/10-0.5,len(moving_average)),moving_average)\n",
    "\n",
    "optimal_epoch = 0.6 + (moving_average.index(max(moving_average))/10)\n",
    "\n",
    "plt.title(\"Best avg dev accuracy = {0:.2f}%, achieved at epoch = {1:.1f}\".format(max(moving_average), optimal_epoch),\\\n",
    "         fontsize=20)\n",
    "plt.xticks(range(round(len(dev_accuracies)/10)))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
